{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63eac0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca27ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.text\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'class':'a-offscreen'}).text\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        # try:\n",
    "        #     # If there is some deal price\n",
    "        #     price = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    "\n",
    "        # except:\n",
    "            price = \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\t\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\t\n",
    "\n",
    "    return review_count\n",
    "\n",
    "# Function to extract Availability Status\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        available = \"Not Available\"\t\n",
    "\n",
    "    return available\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4713b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     # add your user agent \n",
    "#     HEADERS = ({'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "#     # The webpage URL\n",
    "#     URL = \"https://www.amazon.in/s?k=phone&page=3&crid=3FN3QQVMHN0XN&qid=1746291295&sprefix=phon%2Caps%2C238&xpid=qx-CZbB6-vUkk&ref=sr_pg_3\"\n",
    "\n",
    "#     # HTTP Request\n",
    "#     webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "#     # Soup Object containing all data\n",
    "#     soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "#     # Fetch links as List of Tag Objects\n",
    "#     links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-line-clamp-2 s-link-style a-text-normal'})\n",
    "\n",
    "#     # Store the links\n",
    "#     links_list = []\n",
    "\n",
    "#     # Loop for extracting links from Tag Objects\n",
    "#     for link in links:\n",
    "#             links_list.append(link.get('href'))\n",
    "\n",
    "#     d = {\"title\":[], \"price\":[], \"rating\":[], \"reviews\":[],\"availability\":[]}\n",
    "    \n",
    "#     # Loop for extracting product details from each link \n",
    "#     for link in links_list:\n",
    "#         new_webpage = requests.get(\"https://www.amazon.in\" + link, headers=HEADERS)\n",
    "\n",
    "#         new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "#         # Function calls to display all necessary product information\n",
    "#         d['title'].append(get_title(new_soup))\n",
    "#         d['price'].append(get_price(new_soup))\n",
    "#         d['rating'].append(get_rating(new_soup))\n",
    "#         d['reviews'].append(get_review_count(new_soup))\n",
    "#         d['availability'].append(get_availability(new_soup))\n",
    "\n",
    "    \n",
    "#     amazon_df = pd.DataFrame.from_dict(d)\n",
    "#     amazon_df['title'].replace('', np.nan, inplace=True)\n",
    "#     amazon_df = amazon_df.dropna(subset=['title'])\n",
    "#     amazon_df.to_csv(\"amazon_data.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db08fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4413e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1...\n",
      "Scraping Page 2...\n",
      "Scraping Page 3...\n",
      "Scraping Page 4...\n",
      "Scraping Page 5...\n",
      "Scraping Page 6...\n",
      "Scraping Page 7...\n",
      "Scraping Page 8...\n",
      "Scraping Page 9...\n",
      "Scraping Page 10...\n",
      "\n",
      "âœ… Scraping Completed. Saved 0 products to ../data/amazon_smart home_data.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/zsh7smnd15z_gk8v12njmtd00000gn/T/ipykernel_54335/2648671959.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  amazon_df['title'].replace('', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time,os\n",
    "import random\n",
    "\n",
    "# Your existing functions (get_title, get_price, etc.) stay the same here!\n",
    "\n",
    "# Function to build Amazon search URL dynamically\n",
    "def build_amazon_search_url(search_term, page=1):\n",
    "    search_term = search_term.replace(' ', '+')\n",
    "    return f\"https://www.amazon.in/s?k={search_term}&page={page}\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Set headers\n",
    "    HEADERS = ({\n",
    "        'User-Agent': '<user-agent>',\n",
    "        'Accept-Language': 'en-US,en;q=0.9'\n",
    "    })\n",
    "\n",
    "    # ðŸ”¥ Input from user\n",
    "    search_keyword = input(\"Enter product to search on Amazon: \").strip()\n",
    "    total_pages = int(input(\"Enter number of pages to scrape: \"))\n",
    "\n",
    "    d = {\"title\":[], \"price\":[], \"rating\":[], \"reviews\":[],\"availability\":[]}\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        print(f\"Scraping Page {page}...\")\n",
    "        URL = build_amazon_search_url(search_keyword, page)\n",
    "        webpage = requests.get(URL, headers=HEADERS)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-line-clamp-2 s-link-style a-text-normal'})\n",
    "        links_list = [link.get('href') for link in links]\n",
    "\n",
    "        for link in links_list:\n",
    "            product_url = \"https://www.amazon.in\" + link\n",
    "            new_webpage = requests.get(product_url, headers=HEADERS)\n",
    "            new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "            d['title'].append(get_title(new_soup))\n",
    "            d['price'].append(get_price(new_soup))\n",
    "            d['rating'].append(get_rating(new_soup))\n",
    "            d['reviews'].append(get_review_count(new_soup))\n",
    "            d['availability'].append(get_availability(new_soup))\n",
    "\n",
    "            time.sleep(random.uniform(1, 3))  # Random sleep\n",
    "\n",
    "    # Save results\n",
    "    amazon_df = pd.DataFrame.from_dict(d)\n",
    "    amazon_df['title'].replace('', np.nan, inplace=True)\n",
    "    amazon_df = amazon_df.dropna(subset=['title'])\n",
    "\n",
    "    # Save file with dynamic name\n",
    "    filename = f\"../data/amazon_{search_keyword}_data.xlsx\"\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    amazon_df.to_excel(filename, index=False)\n",
    "\n",
    "    print(f\"\\nâœ… Scraping Completed. Saved {len(amazon_df)} products to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ebf0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
